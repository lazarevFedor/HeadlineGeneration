import torch
import pandas as pd
import re
from datetime import datetime
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import (GPT2Tokenizer, GPT2LMHeadModel,
                          DataCollatorForLanguageModeling,
                          TrainingArguments, Trainer)
import shutil


def main():
    DS_AMOUNT = 75000  # –∫–æ–ª-–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    OUTPUT_DIR = "./modelFolder"

    # –õ–æ–≥–≥–µ—Ä
    def log(msg):
        print(f"{datetime.now()} : {msg}")

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞;
    # –†–∞–±–æ—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ —Å –ö–∏—Ä–∏–ª–ª–∏—Ü–µ–π –∏ –∑–Ω–∞–∫–∞–º–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è;
    def clean_text(text):
        if not isinstance(text, str):
            return ""
        text = re.sub(r"[^–∞-—è–ê-–Ø—ë–Å .,!?]", "", text)  # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ö–∏—Ä–∏–ª–ª–∏—Ü—É
        text = re.sub(r"\s+", " ", text).strip()  # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ 1
        return text

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –Ω–∞–ª–∏—á–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–π —Ä—É—Å—Å–∫–æ–π –±—É–∫–≤—ã –≤ —Ç–µ–∫—Å—Ç–µ
    def has_letters(text):
        return bool(re.search(r"[–∞-—è–ê-–Ø—ë–Å]", text))

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=128
        )

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    def generate_title(news_text, in_model, tknzr):
        input_text = f"–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏: {news_text} [SEP] –ó–∞–≥–æ–ª–æ–≤–æ–∫:"
        input_ids = tknzr.encode(input_text, return_tensors="pt").to(device)

        output = in_model.generate(
            input_ids,
            min_length=5,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            max_new_tokens=20,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            num_beams=10,  # Beam Search –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
            no_repeat_ngram_size=2,  # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
            temperature=0.7,  # –ö–æ–Ω—Ç—Ä–æ–ª—å —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
            top_k=50,  # –£–±–∏—Ä–∞–µ–º –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            top_p=0.9,  # Top-p sampling
            do_sample=True,  # –í–∫–ª—é—á–∞–µ–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            early_stopping=True  # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ —Ç–æ–∫–µ–Ω–∞ –∫–æ–Ω—Ü–∞
        )

        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞
        gen_text = tknzr.decode(output[0], skip_special_tokens=True)
        gen_title = gen_text.split("–ó–∞–≥–æ–ª–æ–≤–æ–∫:")[-1].strip()
        return gen_title

    file_path = "/kaggle/input/corpus-of-russian-news-articles-from-lenta/lenta-ru-news.csv"

    df = pd.read_csv(file_path, low_memory=False)
    log("–î–∞—Ç–∞—Å–µ—Ç —Å—á–∏—Ç–∞–Ω")

    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: 'title' –∏ 'text'
    df = df[['title', 'text']]
    log("–ò–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ —É–¥–∞–ª–µ–Ω–∞ –ª–∏—à–Ω—è—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")

    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    df['text'] = df['text'].apply(clean_text)
    df['title'] = df['title'].apply(clean_text)

    # –ò–∑–±–∞–≤–ª—è–µ–º—Å—è –æ—Ç –Ω–µ–Ω—É–∂–Ω–æ–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    df = df[df['text'].apply(has_letters) & df['title'].apply(has_letters)]
    log("–î–∞—Ç–∞—Å–µ—Ç –æ—á–∏—â–µ–Ω")

    # –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    df = df.reset_index(drop=True)
    log("–î–∞—Ç–∞—Å–µ—Ç –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

    df['input'] = "–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏: " + df['text'] + " [SEP] –ó–∞–≥–æ–ª–æ–≤–æ–∫:"
    df['target'] = df['title']
    df = df.sample(DS_AMOUNT, random_state=42)
    log(f"–í—ã–±—Ä–∞–Ω–æ {DS_AMOUNT} –≤—Ö–æ–¥–Ω—ã—Ö –∏ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ –≤—ã–±–æ—Ä–∫–∏
    train_data, val_data = train_test_split(df, test_size=0.1, random_state=42)

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç Dataset
    train_dataset = Dataset.from_dict({"text": train_data['input'].tolist(),
                                       "label": train_data['target'].tolist()})
    val_dataset = Dataset.from_dict({"text": val_data['input'].tolist(),
                                     "label": val_data['target'].tolist()})

    log("–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–∞–±–ª–∏—á–∫–∏ –≤ Dataset object")
    log(f"–ü—Ä–∏–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–∞–±–æ—Ä–∞ : {train_dataset[0]}")

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokenizer = GPT2Tokenizer.from_pretrained("sberbank-ai/rugpt3small_based_on_gpt2")

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω –ø–∞–¥–¥–∏–Ω–≥–∞ —Ä–∞–≤–Ω—ã–º —Ç–æ–∫–µ–Ω—É –∫–æ–Ω—Ü–∞ —Ç–µ–∫—Å—Ç–∞
    tokenizer.pad_token = tokenizer.eos_token
    log("–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    train_dataset = train_dataset.map(tokenize_function, batched=True)
    val_dataset = val_dataset.map(tokenize_function, batched=True)
    log("–î–∞—Ç–∞—Å–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω")

    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    train_dataset = train_dataset.remove_columns(["text", "label"])
    val_dataset = val_dataset.remove_columns(["text", "label"])
    log("–ò–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ —É–±—Ä–∞–Ω—ã –ª–∏—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ(—Å—Ç–æ–ª–±—Ü—ã)")

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ PyTorch
    train_dataset.set_format("torch")
    val_dataset.set_format("torch")
    log("–î–∞—Ç–∞—Å–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ —Ç–µ–Ω–∑–æ—Ä—ã –¥–ª—è –ø–æ–¥–∞—á–∏ –≤ –º–æ–¥–µ–ª—å")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ Data collator
    model = GPT2LMHeadModel.from_pretrained("sberbank-ai/rugpt3small_based_on_gpt2")
    log("–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –∏–Ω—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    log("–ë–∞—Ç—á–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –≤ collator")

    training_args = TrainingArguments(
        ddp_find_unused_parameters=False,
        output_dir=OUTPUT_DIR,
        eval_strategy="epoch",
        logging_strategy="epoch",
        save_strategy="epoch",
        num_train_epochs=8,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=8,
        learning_rate=5e-5,
        warmup_steps=500,
        weight_decay=0.01,
        report_to="none",
        fp16=True,
        load_best_model_at_end=True
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator
    )

    log("–¢—Ä–µ–Ω–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")

    trainer.train()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    print("üîç 2.–ò—Å–ø–æ–ª—å–∑—É–µ–º DDP:", torch.distributed.is_initialized())
    print("üß† 2.–¢–µ–∫—É—â–∏–π —Ä–∞–Ω–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞:", torch.distributed.get_rank() if torch.distributed.is_initialized() else "N/A")

    article = (
        "–†–æ—Å—Å–∏—è –≤ –±–ª–∏–∂–∞–π—à–∏–µ –≥–æ–¥—ã –±—É–¥–µ—Ç –Ω–∞—Ä–∞—â–∏–≤–∞—Ç—å –æ–±—ä–µ–º —Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —Å—Ñ–µ—Ä–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, "
        "–∑–∞—è–≤–∏–ª –≤–∏—Ü–µ-–ø—Ä–µ–º—å–µ—Ä –†–§ –î–º–∏—Ç—Ä–∏–π –ß–µ—Ä–Ω—ã—à–µ–Ω–∫–æ. –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç ‚Äì —ç—Ç–æ —Ç–∞ –ø—Ä–æ—Ä—ã–≤–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è, "
        "–∫–æ—Ç–æ—Ä–∞—è –≤–∞–∂–Ω–∞ –∫–∞–∫ –¥–ª—è –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏—Ö, —Ç–∞–∫ –∏ –¥–ª—è –≤–æ–µ–Ω–Ω—ã—Ö –Ω—É–∂–¥."
    )

    generated_title = generate_title(article, model, tokenizer)
    print("–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏:", article)
    print("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫:", generated_title)

    # —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∏–Ω –≤–µ—Ä—Å–∏—é, –Ω–æ –º–æ–∂–Ω–æ –∏ –±–µ–∑ –Ω–µ–µ, —Ç–∫ —Ç—Ä–µ–Ω–µ—Ä –∞–≤—Ç–æ–º–∞—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é –ª—É—á—à—É—é –≤–µ—Ä—Å–∏—é
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    # –ê—Ä—Ö–∏–≤–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ ZIP-–∞—Ä—Ö–∏–≤
    shutil.make_archive("trained_model", 'zip', OUTPUT_DIR)
    log("–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∏ –∑–∞–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∞ –∫–∞–∫ trained_model.zip")

    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    # –ê—Ä—Ö–∏–≤–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ ZIP-–∞—Ä—Ö–∏–≤
    shutil.make_archive("trained_model", 'zip', OUTPUT_DIR)
    log("–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∏ –∑–∞–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∞ –∫–∞–∫ trained_model.zip")


if __name__ == "__main__":
    main()